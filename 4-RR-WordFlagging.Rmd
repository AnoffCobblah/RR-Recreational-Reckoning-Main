---
title: "4-RR-WordFlagging"
author: "Anoff Nicholas Cobblah"
date: "July 30, 2018"
output: html_document
    number_sections: yes
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


*********************************************************************************************************************************************
*********************************************************************************************************************************************
*********************************************************************************************************************************************
# Word Flagging

In what I call *word flagging*, I note where a certain word (or words) appear in a text (or texts), and visualizing those appearances. For instance, this technique could tell you visually whether Esther is mentioned more by name in the beginning or at the end of *Bleak House*. Or it could show where the words "North" and "South" appear in Gaskell's *North and South*, which might be useful in studying how the narrative makes use of those spaces.

As usual, the first step is again to create a list of words you are interested in locating. **Note that the object "flagterms" must be a vector of single word character strings.

```{r flagterms, eval=FALSE}
  flagterms <- c("up", "down", "strange", "charm", "top", "bottom")
```

Please note that this script also requires the existence of Processed Texts.Once you have processed texts uploaded to a folder, you can specify that these are the files you want to look at using the input below.

```{r files, eval=FALSE}
files <- list.files(path = ProcessedDataLocation, pattern = "txt", full.names = TRUE)
```


## Quick Word Flag Script

Flagging where these terms occur within the text means that, unlike other types of analyses, this script cannot treat the text as a "bag of words": we need to create a matrix (labelled data) which maintains information not only on whether or not the terms are in the text, but also where they appear. I've created a function for this, as this is a feature I use fairly frequently. When the function is flagging lemma (individual words), I use the function "WordFlagLemmas()". It outputs a dataframe called "WordFlagdf".

```{r Word Flag, eval=FALSE}
WordFlagLemmas(files = files, flagterms = flagterms)
WordFlagdf
```

## Slow, Thorough Sentence Level Word Flag Script

Unlike the previous function, which immediately transforms the document into a list of words, the function LSWordFlag() preserves the sentences which the lemma match came from. For this reason, it is SIGNIFICANLY slower. However, often it is nice to have this data available for key words in context. The data is put into a dataframe labeled "LSWordFlagdf".

```{r Word Flag Sentence Slow, eval=FALSE}
WordFlagLS(files = files, flagterms = flagterms)
LSWordFlagdf

```

## Fast, Sentence Level Word Flag Script
This function, "WordFlagSentences" is a bit faster, because it doesn't attempt to keep track of the lemma %. Instead, it judges based on the sentences in which the match appears. It outputs a dataframe called "SentenceWordFlagdf".

```{r Word Flag Sentence Fast, eval=FALSE}
WordFlagSentences(files = files,flagterms = flagterms)
SentenceWordFlagdf

```

There are a lot of things that you can do with these word flag data sets. Sometimes you want to know where a set of words occurs in just one text, sometimes you want to compare their use across a corpus. In addition, because it creates a data point for every reference to a word, this data can also be added up to determine a word's *frequency* in a text or corpus. Because there are often multiple uses for this data, it is helpful to be able to save it. You can save the Word Flag data set using the script below. It will create a new folder for the output as well.

```{r Word Flagging Output Location, eval=FALSE}
  wfoutputlocation <- paste0(getwd(),"/","wfoutput","-",format(Sys.Date()))
  if(file.exists(wfoutputlocation) == FALSE) {dir.create(wfoutputlocation)}
  tempname <- paste0(wfoutputlocation,"/","wfdata-",format(Sys.Date()),".txt")
  write.table(WordFlagdf, file=tempname)
```

To read in previously constructed data, edit wfoutputlocation below and run the script:
```{r Word Flagging input, eval=FALSE}
  wfoutputlocation <- paste0(getwd(),"/","wfoutput","-",format(Sys.Date()))
  tempname <- paste0(wfoutputlocation,"/","wfdata-",format(Sys.Date()),".txt")
  data <- read.table(tempname, header=TRUE)
  WordFlagdf <- as.data.frame(data)
  WordFlagdf
```

## Visualizations of Word Flagging: 

**NOTE THAT ALL THE FOLLOWING VISUALIZATIONS ASSUME THE EXISTENCE OF THE OBJECT "WordFlagdf." Complete the section above if this has not been constructed.

###Scatterplots

Visualizations are hard to automate. I can do some level of predicting, but ultimately the plots below will probably need to be edited to fit new research projects.

#### Scatterplot: 1 Text

The simplest visualization is when one has only one text that they are visualizing. In this case, it is clear that best practice is to have the % of the text (by lemma) on the x-axis and the searched terms on the y-axis.

```{r Scatterplot Single Text, eval=FALSE}
  p <- ggplot(WordFlagdf, aes(y = as.factor(Searched_Term), x = as.numeric(as.character(Lemma_Perc))))
  pg <- geom_point(size=1,pch = 16)
  pl <- p + pg + labs(x = "% of Text (by word)", y = "Searched Term", title = "Appearances of Keywords within Text")
  pl
```

If you want to save this output, it can be done with ggsave function.
```{r Scatterplot Single Text Save, eval=FALSE}
  wfoutputlocation <- paste0(getwd(),"/","wfoutput","-",format(Sys.Date()))
  if(file.exists(wfoutputlocation) == FALSE) {dir.create(wfoutputlocation)}
  tempname <- paste0(wfoutputlocation,"/","wfscatter-",format(Sys.Date()),".png")
  ggsave(tempname, device="png")
```

#### Scatterplot: Multiple texts, 1 Plot per Keyword

Multiple plots are trickier: if you have too many factors for the y axis, they can become cluttered. And while one could THEORETICALLY map all of the keywords on one plot, using different colors, this is not good practice and makes it very hard to interpret. The "facet_wrap()" function can help if have few enough texts.

```{r Scatterplot Multi Text facet, eval=FALSE}
    p <- ggplot(WordFlagdf, mapping = aes(y = as.factor(Text), x = as.numeric(as.character(Lemma_Perc))))
    pg <- geom_point(size=1,pch = 16)
    pl <- p + pg + labs(x = "% of Text (by word)", y = "Text", title = "Appearances of Terms w/i Texts") + facet_wrap(~Searched_Term)
    print(pl)
```

Again if you want to save this output, it can be done with ggsave function.
```{r Scatterplot Multi Text Save, eval=FALSE}
  wfoutputlocation <- paste0(getwd(),"/","wfoutput","-",format(Sys.Date()))
  if(file.exists(wfoutputlocation) == FALSE) {dir.create(wfoutputlocation)}
  tempname <- paste0(wfoutputlocation,"/","wfscatter-",format(Sys.Date()),".png")
  ggsave(tempname, device="png")
```

For more than 10 searchedterms or texts, multiple plots are the best way of going about this:

```{r Scatterplot Multi Text, eval=FALSE}
  for(i in 1:length(flagterms)) {
    temptitle <- paste0("Appearances of '",flagterms[i],"' within Texts")
    tempdatadf <- subset(WordFlagdf,WordFlagdf$Searched_Term == flagterms[i])
    p <- ggplot(tempdatadf, aes(y = as.factor(Text), x = as.numeric(as.character(Lemma_Perc))))
    pg <- geom_point(size=1,pch = 16)
    pl <- p + pg + labs(x = "% of Text (by word)", y = "Text", title = temptitle)
    print(pl)
  }
```

However, if this method isn't working, one can simply subset further. For example, if I decide that I am only interested in Middlemarch and Daniel Deronda, I could subset the data in the following way:
```{r Scatterplot Multi Text subset, eval=FALSE}
  for(i in 1:length(flagterms)) {
    temptitle <- paste0("Appearances of '",flagterms[i],"' within Texts")
    tempdatadf <- WordFlagdf[grep(flagterms[i],WordFlagdf$Searched_Term),]
    p <- ggplot(subset(tempdatadf,tempdatadf$Text %in% c("1863_Middlemarch","1876_Daniel-Deronda")), aes(y = as.factor(Text), x = as.numeric(as.character(Lemma_Perc))))
    pg <- geom_point(size=1,pch = 16)
    pl <- p + pg + labs(x = "% of Text (by word)", y = "Text", title = temptitle)
    print(pl)
  }
```

In this example the extra subset is not very helpful, but in examples with more data, you can also subset using logical operators to take only value before certain dates.

### Term Frequency Visualizations
Instead of visualizing where terms are referenced in a text, you may instead be interested in understanding the frequency with which they appear in certain sections of a text(s), or the overall number of times they occur within a document. This technique could tell you, for instance, whether the word "London" appears more often in early Dickens than late Dickens.  Or it could tell you how often Darwin refers to "man" in *Origin of Species*.

There are many ways of finding term frequencies. The first and most generalizable method is to continue working with the data which was created in Word Flagging, and to create histograms or other types of binned plots with that data. Histograms allow one to see the frequency or probability density with which terms fall in various parts of the text. Similarly, a binned tile plot can be used to create a heat map which represents visually which areas of which texts have the greatest occurrence of a search term.

#### Bar Charts

Obviously if you have already flagged the location where a list of terms appears in a corpus of texts, you can simply add these up to find out how often they appear in a text, as seen below.

```{r, term frequency entire document, eval=FALSE}

  tfTexts <- unique(WordFlagdf$Text)
  tfsearchedterm <- unique(WordFlagdf$Searched_Term)
  tempmat <- matrix(,nrow=length(tfTexts),ncol=length(tfsearchedterm))
  colnames(tempmat) <- tfsearchedterm
  rownames(tempmat) <- tfTexts
  for(i in 1:length(tfTexts)){
    tempdatadf1 <- subset(WordFlagdf,WordFlagdf$Text == tfTexts[i]) #focuses just on one text at a time
    for(j in 1:length(tfsearchedterm)){
      tempdatadf2 <- subset(tempdatadf1,tempdatadf1$Searched_Term == tfsearchedterm[j]) #focuses just on one searched term at a time
      tempmat[i,j] <- nrow(tempdatadf2) 
    }
  }
  tempmat
```

But a faster way of doing this would simply be to use ggplot's barchart function to add these elements together.

```{r, term frequency ggplot, eval=FALSE}
    p <- ggplot(WordFlagdf, mapping = aes(x = as.factor(Text)))
    pg <- geom_bar() 
    pl <- p + pg + labs(x = "Text", title = "Appearances of Terms w/i Texts") + facet_wrap(~Searched_Term) + coord_flip()
    print(pl)
```


#### Histograms
You can also do something else: break the text vector into 'bins' and determine how frequently certain words are referenced in certain parts of the text.  This can be visualized most easily by using the histogram function in R.

Again, these are the types of visualizations which require some forethought, to ensure these many plots are readable. Binwidth can be adjusted as necessary. And the arrangedment of the grid in terms of keywords or texts can easily be changed.

```{r histogram script, eval=FALSE}

  tfTexts <- unique(WordFlagdf$Text)
  tfsearchedterm <- unique(WordFlagdf$Searched_Term)
    for(i in 1:length(tfTexts)){
      tempdatadf1 <- subset(WordFlagdf,WordFlagdf$Text == tfTexts[i]) #focuses just on one text at a time
      p <- ggplot(tempdatadf1, mapping = aes(x = as.numeric(as.character(Lemma_Perc))))
      pg <- geom_histogram(binwidth = 10)
      pl <- p + pg + labs(x = "Text", title =  paste0("Histograms of Terms in ", tfTexts[i])) + facet_wrap(~Searched_Term)
      print(pl)
      }
```

#### Binned Tile Plot

Histograms are an easy way of binning data in R, but for a more visually striking example for comparing how search terms appear across several texts, a binned tile plot can be used to create a heat map which represents visually which areas of which texts have the greatest occurrence of a search term. **Histograms are recommended over heat maps, as they are clearer to read.**

```{r Binned Tile Plot, eval=FALSE}

  tfTexts <- unique(WordFlagdf$Text)
  tfsearchedterm <- unique(WordFlagdf$Searched_Term)
  binnedtilemat <- matrix(,nrow=1,ncol=5)
  for(i in 1:length(tfTexts)) {
    tempdf1 <- subset(WordFlagdf,WordFlagdf$Text == tfTexts[i]) #focuses just on one text at a time
    for(u in 1:length(tfsearchedterm)) {
      blankmat <- matrix(,nrow=20,ncol=5)
      colnames(blankmat) <- c("Searched_Term", "bin", "bin_avg", "frequency","Text")
      tempdf2 <- subset(tempdf1,tempdf1$Searched_Term == tfsearchedterm[u]) #focuses just on one searched term at a time
      blankmat[,1] <- as.character(tfsearchedterm[u])
      blankmat[,2] <- c("(0-5)","(5-10)", "(10-15)", "(15-20)", "(20-25)", "(25-30)", "(30-35)", "(35-40)", "(40-45)", "(45-50)", "(50-55)", "(55-60)", "(60-65)", "(65-70)", "(70-75)", "(75-80)", "(80-85)", "(85-90)", "(90-95)", "(95-100)")
      blankmat[,3] <- c(2.5,7.5,12.5,17.5,22.5,27.5,32.5,37.5,42.5,47.5,52.5,57.5,62.5,67.5,72.5,77.5,82.5,87.5,92.5,97.5)
      blankmat[,4] <- 0 #ran into some trouble here.  Cut doesn't always return the # of bins if no data, so I needed to add it
        bins <- cut(as.numeric(as.character(tempdf2$Lemma_Perc)),breaks=seq(0,100,by=5),labels=FALSE, include.lowest = TRUE)
        for (v in 1:20) { #to do: something wrong here
          mat3 <- as.matrix(table(bins))
          vec3 <- as.numeric(rownames(mat3))
          blankmat[vec3[v],4] <- mat3[v]
        }
      blankmat[,5] <- as.character(tfTexts[i])
      binnedtilemat <- rbind(binnedtilemat,blankmat)
    }
    binnedtilemat <- binnedtilemat[-1,]
    binnedtiledf <- as.data.frame(binnedtilemat)
      p <- ggplot(binnedtiledf, aes(y = as.factor(Searched_Term), x= as.factor(bin))) + geom_tile(aes(fill=as.numeric(as.character(frequency)))) + scale_fill_gradient(low="blue",high="red")
      pl <- p +labs(x = "% of Text (by lemma)", y = "Searched Term", title = paste0("Appearances of Terms in ",tfTexts[i])) + theme(text = element_text(size=8), legend.position = "bottom", legend.title = element_blank())
      print(pl)
      if(i == 1) {
        p <- ggplot(binnedtiledf, aes(y = as.factor(Searched_Term), x= as.factor(bin))) + geom_tile(aes(fill=as.numeric(as.character(frequency)))) + scale_fill_gradient(low="blue",high="red")
      pl <- p +labs(x = "% of Text (by lemma)", y = "Searched Term", title = paste0("Preview: Appearances of Terms in ",tfTexts[i])) + theme(text = element_text(size=8), legend.position = "bottom", legend.title = element_blank())
      print(pl)
      }
  }
```

*********************************************************************************************************************************************
*********************************************************************************************************************************************
*********************************************************************************************************************************************